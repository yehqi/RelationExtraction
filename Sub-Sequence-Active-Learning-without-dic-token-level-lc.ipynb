{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca23166c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import numpy\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "sys.path.append('../BERT-keras/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8e299",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d662be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 路径定义\n",
    "data_origins = './CMeIE/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b1d27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-C-1': 2,\n",
       " 'B-C-2': 4,\n",
       " 'B-DD-1': 14,\n",
       " 'B-DD-2': 16,\n",
       " 'B-PA-1': 6,\n",
       " 'B-PA-2': 8,\n",
       " 'B-RO-1': 10,\n",
       " 'B-RO-2': 12,\n",
       " 'B-RS-1': 22,\n",
       " 'B-RS-2': 24,\n",
       " 'B-RT-1': 18,\n",
       " 'B-RT-2': 20,\n",
       " 'I-C-1': 3,\n",
       " 'I-C-2': 5,\n",
       " 'I-DD-1': 15,\n",
       " 'I-DD-2': 17,\n",
       " 'I-PA-1': 7,\n",
       " 'I-PA-2': 9,\n",
       " 'I-RO-1': 11,\n",
       " 'I-RO-2': 13,\n",
       " 'I-RS-1': 23,\n",
       " 'I-RS-2': 25,\n",
       " 'I-RT-1': 19,\n",
       " 'I-RT-2': 21,\n",
       " 'None': 0,\n",
       " 'O': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Disease_Disease_Label_Mapping = {\n",
    "    '并发症':'C',\n",
    "    '病理分型':'PA',\n",
    "    '相关（导致）':'RO',\n",
    "    '鉴别诊断':'DD',\n",
    "    '相关（转化）':'RT',\n",
    "    '相关（症状）':'RS'\n",
    "}\n",
    "\n",
    "cate = ['O']\n",
    "for item in Disease_Disease_Label_Mapping.values():\n",
    "    cate.append('B-'+ item + '-1')\n",
    "    cate.append('I-'+ item + '-1')\n",
    "    cate.append('B-'+ item + '-2')\n",
    "    cate.append('I-'+ item + '-2')\n",
    "id_list = [i for i in range(1,len(cate)+1)]\n",
    "cate_dict = {item[0]:item[1] for item in zip(cate,id_list)}\n",
    "cate_dict['None'] = 0\n",
    "\n",
    "cate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2216f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_origins + 'CMeIE_train', 'rb') as f_train:\n",
    "    train_text_label = pickle.load(f_train)\n",
    "with open(data_origins + 'CMeIE_dev', 'rb') as f_dev:\n",
    "    dev_text_label = pickle.load(f_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "067bd3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length = [len(item) for item in train_text_label['label']]\n",
    "%matplotlib inline\n",
    "plt.hist(length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42c0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_bert.tokenization import _is_control, _is_whitespace, load_vocab,convert_by_vocab\n",
    "\n",
    "def _is_chinese_char(char):\n",
    "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "    #\n",
    "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "    # space-separated words, so they are not treated specially and handled\n",
    "    # like the all of the other languages.\n",
    "    cp = ord(char)\n",
    "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "            (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "            (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "            (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "            (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "            (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "            (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "            (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def _run_strip_accents(text):\n",
    "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    output = []\n",
    "    for char in text:\n",
    "        cat = unicodedata.category(char)\n",
    "        if cat == \"Mn\":\n",
    "            continue\n",
    "        output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "def _word_piece_tokenize(token, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
    "    chars = list(token)\n",
    "    if len(chars) > max_input_chars_per_word:\n",
    "        return [unk_token], [0 for i in range(len(chars))]\n",
    "\n",
    "    is_bad = False\n",
    "    start = 0\n",
    "    sub_tokens = []\n",
    "    sub_tokens_offset = [0 for i in range(len(chars))]\n",
    "    \n",
    "    while start < len(chars):\n",
    "        end = len(chars)\n",
    "        cur_substr = None\n",
    "        while start < end:\n",
    "            substr = \"\".join(chars[start:end])\n",
    "            if start > 0:\n",
    "                substr = \"##\" + substr\n",
    "            if substr in vocab:\n",
    "                cur_substr = substr\n",
    "                break\n",
    "            end -= 1\n",
    "        if cur_substr is None:\n",
    "            is_bad = True\n",
    "            break\n",
    "        sub_tokens.append(cur_substr)\n",
    "        for i in range(start,end):\n",
    "            sub_tokens_offset[i] = len(sub_tokens) - 1\n",
    "        start = end\n",
    "\n",
    "    if is_bad:\n",
    "        return [unk_token], [0 for i in range(len(chars))]\n",
    "    else:\n",
    "        return sub_tokens, sub_tokens_offset\n",
    "\n",
    "class BERTTextEncoder(object):\n",
    "    def __init__(self, vocab_file: str, do_lower_case: bool = True) -> None:\n",
    "        self.do_lower_case = do_lower_case\n",
    "        \n",
    "        # vocab词表信息\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        # 加载对应\n",
    "        self.bert_pad_id = self.vocab['[PAD]']\n",
    "        # 99 used\n",
    "        self.bert_unk_id = self.vocab['[UNK]']\n",
    "        self.bert_cls_id = self.vocab['[CLS]']\n",
    "        self.bert_sep_id = self.vocab['[SEP]']\n",
    "        self.bert_msk_id = self.vocab['[MASK]']\n",
    "        self.vocab_size = len(self.vocab) - 99 - 5\n",
    "        \n",
    "        \n",
    "    def standardize_ids(self, ids):\n",
    "        for i in range(len(ids)):\n",
    "            if ids[i] == self.bert_pad_id:  # PAD\n",
    "                ids[i] = 1 + self.vocab_size\n",
    "            elif ids[i] == self.bert_unk_id:  # UNK\n",
    "                ids[i] = 0\n",
    "            elif ids[i] == self.bert_cls_id:  # CLS\n",
    "                ids[i] = 3 + self.vocab_size\n",
    "            elif ids[i] == self.bert_sep_id:  # SEP\n",
    "                ids[i] = 5 + self.vocab_size\n",
    "            elif ids[i] == self.bert_msk_id:  # MASK\n",
    "                ids[i] = 2 + self.vocab_size\n",
    "            elif ids[i] > self.bert_msk_id:  # VOCAB\n",
    "                ids[i] -= self.bert_msk_id\n",
    "        return ids\n",
    "\n",
    "    # 返回 tokens, real_token_offset\n",
    "    def tokenize(self, strA):\n",
    "        orig_tokens = []\n",
    "        char_orig_token_offset = []\n",
    "        \n",
    "        is_token_start = True\n",
    "        is_token_end = False\n",
    "        # 初步分词\n",
    "        for char in strA:\n",
    "            cp = ord(char)\n",
    "            \n",
    "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "                char = ''\n",
    "            elif _is_whitespace(char):\n",
    "                is_token_end = True\n",
    "                char = ''\n",
    "            elif _is_chinese_char(char):\n",
    "                is_token_start = True\n",
    "                is_token_end = True\n",
    "                \n",
    "                \n",
    "            if is_token_start:\n",
    "                if len(orig_tokens) == 0 or len(orig_tokens[-1]) != 0: \n",
    "                    orig_tokens.append('')\n",
    "                is_token_start = False\n",
    "            \n",
    "            if char == '':\n",
    "                char_orig_token_offset.append(None)\n",
    "            else:\n",
    "                char_orig_token_offset.append((len(orig_tokens) - 1, len(orig_tokens[-1])))\n",
    "                orig_tokens[-1] += char\n",
    "        \n",
    "            if is_token_end:\n",
    "                is_token_start = True\n",
    "                is_token_end = False\n",
    "        \n",
    "        if orig_tokens[-1] == '':\n",
    "            orig_tokens.pop()\n",
    "        \n",
    "        \n",
    "        # 详细分词\n",
    "        \n",
    "        split_tokens = []\n",
    "        split_tokens_map = {}\n",
    "        \n",
    "        for idx, token in enumerate(orig_tokens):\n",
    "            if self.do_lower_case:\n",
    "                token = token.lower()\n",
    "                token = _run_strip_accents(token)\n",
    "            \n",
    "            sub_tokens, sub_tokens_offset = _word_piece_tokenize(token, self.vocab)\n",
    "            \n",
    "            for o_idx, offset in enumerate(sub_tokens_offset):\n",
    "                split_tokens_map[(idx, o_idx)] =  len(split_tokens) + offset\n",
    "            \n",
    "            split_tokens.extend(sub_tokens)\n",
    "            \n",
    "        \n",
    "        real_token_offset = []\n",
    "        \n",
    "        for offset in char_orig_token_offset:\n",
    "            if offset is None:\n",
    "                real_token_offset.append(None)\n",
    "            elif offset in split_tokens_map:\n",
    "                real_token_offset.append(split_tokens_map[offset])\n",
    "            else:\n",
    "                real_token_offset.append(None)\n",
    "                \n",
    "        \n",
    "        return split_tokens, real_token_offset\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_by_vocab(self.vocab, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac40eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../BERT-keras/google_bert/tokenization.py:74: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bert 路径定义\n",
    "bert_parameter_location = '../BERT-keras/google_bert/downloads/chinese_L-12_H-768_A-12/'\n",
    "\n",
    "# 文本最长大小\n",
    "max_len = 128\n",
    "\n",
    "# bert解码器引入\n",
    "bert_encoder = BERTTextEncoder(os.path.join(bert_parameter_location, 'vocab.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22746b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you want sentencepiece encoder, please install sentencepiece\n",
      "if you want to use OpenAI's encoder and pretrained model, please install spacy, and ftfy\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import generate_pos_ids\n",
    "\n",
    "def ner_prepare(input_texts, output_labs, bert_encoder, cate_dict, max_len):\n",
    "    input_ids = []\n",
    "    input_masks = []\n",
    "    input_type_ids = np.zeros((len(input_texts), max_len), dtype=np.int32)\n",
    "    output_ids = []\n",
    "    \n",
    "    for input_text,output_lab in tqdm(zip(input_texts,output_labs)):\n",
    "        try:\n",
    "            tokens, char_to_word_offset = bert_encoder.tokenize(input_text)\n",
    "        \n",
    "            if len(tokens) > max_len - 2: #删除多余字符\n",
    "                tokens = tokens[0:(max_len - 2)]       \n",
    "\n",
    "            order = {\"O\":1,'I':2,\"B\":3} #维护一个优先级，对tokens倒序遍历，\n",
    "            output = [\"O\" for i in range(len(tokens))]\n",
    "#             output = ['O'] * len(tokens)\n",
    "        \n",
    "            for idx in range(len(output_lab)):\n",
    "                real_idx = char_to_word_offset[idx]\n",
    "                if real_idx != None: #若某个char是空格等，对应到token中会删去，char_to_word_offset对应位置存储为None。\n",
    "                    if real_idx >= max_len - 2: #超出最大长度部分不考虑\n",
    "                        break\n",
    "                    if order[output_lab[idx][0]] > order[output[real_idx][0]]:#防止存在tokens和人标的边界不一致的情况，导致标签错误。eg，tokens中“1000元”，人标“000元”\n",
    "                        output[real_idx] = output_lab[idx]\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(input_text)\n",
    "            raise e\n",
    "\n",
    "\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]'] # 拼接开始字符，结束字符\n",
    "        input_id = bert_encoder.standardize_ids(bert_encoder.convert_tokens_to_ids(tokens))\n",
    "        \n",
    "        output = ['O'] + output + ['O']\n",
    "        output_id =  [cate_dict[item] if item in cate_dict else cate_dict['O'] for item in output]\n",
    "\n",
    "        input_mask = [1] * (len(tokens))\n",
    "\n",
    "        input_id += [0] * (max_len - len(tokens))\n",
    "        output_id += [0] * (max_len - len(tokens))\n",
    "        input_mask += [0] * (max_len - len(tokens))\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        output_ids.append(output_id)\n",
    "        input_masks.append(input_mask)\n",
    "        \n",
    "    input_ids=np.array(input_ids, dtype=np.int32)\n",
    "    output_ids=np.array(output_ids, dtype=np.int32)\n",
    "    input_masks=np.array(input_masks, dtype=np.int32)\n",
    "        \n",
    "    pos = generate_pos_ids(len(input_texts), max_len)\n",
    "#     attention_mask = create_attention_mask(input_masks, False, None, None, True) \n",
    "    \n",
    "    return [input_ids, input_type_ids, pos, input_masks], output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9ad66aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2842it [00:01, 2030.53it/s]\n",
      "693it [00:00, 2169.44it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = ner_prepare(train_text_label['text'], train_text_label['label'], bert_encoder, cate_dict, max_len)\n",
    "x_dev, y_dev = ner_prepare(dev_text_label['text'], dev_text_label['label'], bert_encoder, cate_dict, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4612ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[:,:,np.newaxis]\n",
    "y_dev = y_dev[:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea412f24",
   "metadata": {},
   "source": [
    "# 评测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12e1b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tag = {value: key for key, value in cate_dict.items()}\n",
    "\n",
    "values = bert_encoder.standardize_ids(list(bert_encoder.vocab.values()))\n",
    "keys = list(bert_encoder.vocab.keys())\n",
    "index_char = {value: key for key, value in zip(keys, values)}\n",
    "id2label_dict = {value:key for key,value in cate_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b60c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测结果\n",
    "from seqeval.metrics import classification_report,f1_score\n",
    "\n",
    "def calculate_PRF(y_val,y_pred,id2label_dict):\n",
    "    true_val = np.squeeze(y_val)\n",
    "    true_val = [[id2label_dict[w] if w != 0 else 'O' for w in s] for s in true_val] \n",
    "\n",
    "#     pred_val = model.predict(x_val)\n",
    "    pred_val = np.argmax(y_pred,axis = -1)\n",
    "    pred_val = [[id2label_dict[w] if w != 0 else 'O' for w in s] for s in pred_val] \n",
    "    \n",
    "    report = classification_report(true_val , pred_val)\n",
    "    f1 = f1_score(true_val , pred_val)\n",
    "    write2file(report)\n",
    "    print(report)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfae7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ens_from_label(label):\n",
    "#         print(label)\n",
    "    label = label + ['O']#为了解决最后一个字符是B或者I的情况\n",
    "    en_start = -1\n",
    "    en_type = \"\"\n",
    "    ens = []\n",
    "    for idx,lab in enumerate(label):\n",
    "#             print(label)\n",
    "        if lab[0] == 'B' and en_start == -1:\n",
    "            en_start = idx\n",
    "            en_type = lab[2:]\n",
    "        elif lab[0] == 'B' and en_start != -1: #解决两个连续实体的情况，中间没有‘O’\n",
    "            ens.append((en_type,en_start,idx))\n",
    "            en_start = idx\n",
    "            en_type = lab[2:]\n",
    "        elif lab[0] == 'I' and en_start == -1:\n",
    "#             print (\"标签序列错误,I不可能是起始标签。\")\n",
    "            continue\n",
    "        elif lab[0] == 'I' and en_start != -1 and lab[2:]!=en_type:\n",
    "            ens.append((en_type,en_start,idx))\n",
    "            en_start = -1\n",
    "        elif lab[0] == 'O' and en_start == -1:\n",
    "            pass\n",
    "        elif lab[0] == 'O' and en_start != -1 :\n",
    "            ens.append((en_type,en_start,idx))\n",
    "            en_start = -1\n",
    "    return ens\n",
    "\n",
    "def char_to_word(char_to_word_offset): \n",
    "    word_to_char_offset = {}\n",
    "    char_range = []\n",
    "    temp_id = char_to_word_offset[0]\n",
    "    for char_id, word_id in enumerate(char_to_word_offset+[-1]):\n",
    "        if word_id != temp_id:\n",
    "            word_to_char_offset[temp_id] = char_range\n",
    "            char_range = []\n",
    "            char_range.append(char_id)\n",
    "        else:\n",
    "            char_range.append(char_id)\n",
    "        temp_id = word_id\n",
    "    return word_to_char_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0df31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ens_list(input_val,pred_lab):\n",
    "    char_ens_list = [] #存储实体列表，shape = (sample_num,ens_num,4),格式为[[(en_type,char_start,char_end,en_text)]]\n",
    "    for item in tqdm(zip(input_val,pred_lab)):\n",
    "#         print(item)\n",
    "        char_ens = []\n",
    "\n",
    "        text = item[0]\n",
    "        pred_lab = item[1] #为set类型\n",
    "\n",
    "        tokens, char_to_word_offset = bert_encoder.tokenize(text)\n",
    "        word_to_char_offset = char_to_word(char_to_word_offset)\n",
    "\n",
    "        pred_lab = pred_lab[1:-1]  #去掉CLS和SEP位置对应的'O'标签\n",
    "\n",
    "        ens = get_ens_from_label(pred_lab)\n",
    "\n",
    "        #对实体列表进行遍历，找到预测实体对应的原文\n",
    "        for en_type,en_start,en_end in ens:\n",
    "    #         if en_start >= len(tokens) or en_end-1> len(tokens):\n",
    "    #             print(\"预测结果将None也预测成了BIO标签\")\n",
    "    #             continue\n",
    "\n",
    "            try:\n",
    "                char_start = word_to_char_offset[en_start][0]\n",
    "                char_end = word_to_char_offset[en_end -1][-1] +1 #位置均为左闭右开\n",
    "            except Exception as e2:\n",
    "                print(len(pred_lab))\n",
    "    #             print(text)\n",
    "                print(len(tokens))\n",
    "    #             print(ens)\n",
    "    #             print(len(word_to_char_offset))\n",
    "    #             print(word_to_char_offset)\n",
    "    #             print(en_end-1)\n",
    "                continue\n",
    "                raise \n",
    "\n",
    "            en_text = text[char_start:char_end]\n",
    "            char_ens.append((en_type,char_start,char_end,en_text))\n",
    "        char_ens_list.append(char_ens)\n",
    "    return char_ens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82c6f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_relations(pred_ens_list):\n",
    "    \n",
    "    pred_relation_list = []\n",
    "    for item in pred_ens_list:\n",
    "        tmp_relation_list = []\n",
    "        label = list(set(list(map(lambda x:x[0].split('-')[0],item))))\n",
    "        label_content = list(map(lambda x:(x[0],x[-1]),item))\n",
    "        all_label_content_set = []\n",
    "        for tmp in label_content:\n",
    "            if tmp not in all_label_content_set:\n",
    "                all_label_content_set.append(tmp)\n",
    "\n",
    "        for relation in label:\n",
    "            label_content_set = list(filter(lambda x:x[0].split('-')[0] == relation,all_label_content_set))\n",
    "            subject_count = len(list(filter(lambda x:x[0].split('-')[-1] == '1',label_content_set)))\n",
    "            object_count = len(list(filter(lambda x:x[0].split('-')[-1] == '2',label_content_set)))\n",
    "\n",
    "            if subject_count !=0 and object_count != 0:\n",
    "                final_count = max(subject_count,object_count)\n",
    "            else:\n",
    "                final_count = 0\n",
    "\n",
    "            tmp_list = []\n",
    "\n",
    "\n",
    "            while subject_count != 1 and object_count != 1 and subject_count !=0 and object_count != 0:\n",
    "                flag = False\n",
    "                subject_object_flag = False\n",
    "                for index,tmp in enumerate(label_content_set):\n",
    "                    if tmp[0].split('-')[-1] == '1':\n",
    "                        flag = True\n",
    "                        subject_index = index\n",
    "                    if tmp[0].split('-')[-1] == '2' and flag:\n",
    "                        tmp_list.append([label_content_set[subject_index],tmp])\n",
    "                        label_content_set = label_content_set[:subject_index]+label_content_set[subject_index+1:index]+label_content_set[index+1:]\n",
    "                        flag = False\n",
    "                        subject_count = len(list(filter(lambda x:x[0].split('-')[-1] == '1',label_content_set)))\n",
    "                        object_count = len(list(filter(lambda x:x[0].split('-')[-1] == '2',label_content_set)))\n",
    "                        subject_object_flag = True\n",
    "                        break\n",
    "\n",
    "                if not subject_object_flag:\n",
    "                    for index,tmp in enumerate(label_content_set):\n",
    "                        if tmp[0].split('-')[-1] == '2':\n",
    "                            flag = True\n",
    "                            object_index = index\n",
    "                        if tmp[0].split('-')[-1] == '1' and flag:\n",
    "                            tmp_list.append([tmp,label_content_set[object_index]])\n",
    "                            label_content_set = label_content_set[:object_index]+label_content_set[object_index+1:index]+label_content_set[index+1:]\n",
    "                            flag = False\n",
    "                            subject_count = len(list(filter(lambda x:x[0].split('-')[-1] == '1',label_content_set)))\n",
    "                            object_count = len(list(filter(lambda x:x[0].split('-')[-1] == '2',label_content_set)))\n",
    "                            break\n",
    "            tmp_list.append(label_content_set)\n",
    "\n",
    "            for lists in tmp_list:\n",
    "                for tmp in itertools.combinations(lists, 2):\n",
    "                    if tmp[0][0].split('-')[-1] != tmp[-1][0].split('-')[-1]:\n",
    "                        tmp_relation_list.append((tmp[0][-1],tmp[-1][-1],relation))\n",
    "\n",
    "        pred_relation_list.append(tmp_relation_list)\n",
    "        \n",
    "    return pred_relation_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8dd419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(relation_list,pred_relation_list,true_relation_list):\n",
    "    dic = {}\n",
    "    all_score = {}\n",
    "    all_precision = float(0)\n",
    "    all_recall = float(0)\n",
    "    all_f = float(0)\n",
    "    for relation in relation_list:\n",
    "        pred_count = 0\n",
    "        true_count = 0\n",
    "        true_pred_count = 0\n",
    "        for i in range(len(pred_relation_list)):\n",
    "            \n",
    "            pred_relation = list(filter(lambda x:x[-1] == relation if x else [], pred_relation_list[i]))\n",
    "            true_relation = list(filter(lambda x:x[-1] == relation if x else [], true_relation_list[i]))\n",
    "            \n",
    "            pred_count+=len(pred_relation)\n",
    "            true_count+=len(true_relation)\n",
    "            true_pred = [item for item in pred_relation if item in true_relation]\n",
    "            true_pred_count+=len(true_pred)\n",
    "          \n",
    "                \n",
    "        if pred_count==0:\n",
    "            p = 0\n",
    "        else:\n",
    "            p = true_pred_count/pred_count*100\n",
    "        if true_count==0:\n",
    "            r = 0\n",
    "        else:\n",
    "            r = true_pred_count/true_count*100\n",
    "        if p == 0 or r == 0:\n",
    "            f = 0\n",
    "        else:\n",
    "            f = 2*p*r/(p+r)\n",
    "        support = true_count\n",
    "        dic[relation] = {\n",
    "            'precision':'%.2f' %(p),\n",
    "            'recall':'%.2f' %(r),\n",
    "            'f1-score':'%.2f' %(f),\n",
    "            'support':support,\n",
    "        }\n",
    "    sum_support = sum(list(map(lambda x:x['support'],dic.values())))\n",
    "    p_s = list(map(lambda x:(x['precision'],x['support']),dic.values()))\n",
    "    r_s = list(map(lambda x:(x['recall'],x['support']),dic.values()))\n",
    "    f_s = list(map(lambda x:(x['f1-score'],x['support']),dic.values()))\n",
    "    for j in range(len(p_s)):\n",
    "        all_precision+=float(float(p_s[j][0])*p_s[j][1]/sum_support)\n",
    "    all_precision = '%.2f' %(all_precision)\n",
    "    for k in range(len(r_s)):\n",
    "        all_recall+=float(float(r_s[k][0])*r_s[k][1]/sum_support)\n",
    "    all_recall = '%.2f' %(all_recall)\n",
    "    for l in range(len(f_s)):\n",
    "        all_f+=float(float(f_s[l][0])*f_s[l][1]/sum_support)\n",
    "    all_f = '%.2f' %(all_f)\n",
    "\n",
    "    classification_report = '          precision    recall  f1-score   support\\n\\n'\n",
    "    for relation in dic:\n",
    "        if len(relation)>=4:\n",
    "            classification_report+=str(relation+' '*(10-len(relation))+str(dic[relation]['precision'])+' '*(11-len(str(dic[relation]['precision'])))+str(dic[relation]['recall'])+' '*(11-len(str(dic[entity]['recall'])))+str(dic[relation]['f1-score'])+' '*(11-len(str(dic[relation]['f1-score'])))+str(dic[relation]['support'])+'\\n')\n",
    "        else:\n",
    "            classification_report+=str('  '+relation+' '*(10-len(relation))+str(dic[relation]['precision'])+' '*(11-len(str(dic[relation]['precision'])))+str(dic[relation]['recall'])+' '*(11-len(str(dic[relation]['recall'])))+str(dic[relation]['f1-score'])+' '*(11-len(str(dic[relation]['f1-score'])))+str(dic[relation]['support'])+'\\n')\n",
    "    classification_report+='\\navg score'+  ' '*(14-len('avg score'))+str(all_precision)+ ' '*(11-len(str(all_precision)))+str(all_recall)+ ' '*(11-len(str(all_recall)))+str(all_f)+' '*(11-len(str(all_f)))+str(sum_support)    \n",
    "    \n",
    "    dic['avg_score'] = {\n",
    "        'precision':all_precision,\n",
    "        'recall':all_recall,\n",
    "        'f1-score':all_f,\n",
    "        'support':sum_support\n",
    "    }\n",
    "    \n",
    "    return dic,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad533f",
   "metadata": {},
   "source": [
    "# 主动学习流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a0e2c",
   "metadata": {},
   "source": [
    "# 定义base model为BERT-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3bee116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From ../BERT-keras/transformer/layers.py:75: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformer.load import load_google_bert\n",
    "\n",
    "bert_model = load_google_bert(bert_parameter_location,max_len = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acdad416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "\n",
    "class BertMask(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(BertMask, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, inputs, input_mask=None):\n",
    "#         print(inputs, input_mask)\n",
    "#         mask = K.squeeze(inputs[1], axis=1)\n",
    "#         return K.equal(mask[:,0], 1)\n",
    "        return K.equal(inputs[1], 1)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        return inputs[0]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "299c05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "\n",
    "\n",
    "def bert_crf():\n",
    "    bert_mask = BertMask()([bert_model.output, bert_model.input[3]])\n",
    "\n",
    "    dense1 = Dense(768,activation = 'tanh')(bert_mask)\n",
    "\n",
    "    # bilstm = Bidirectional(LSTM(int(bert_model.output.shape[-1])//2, return_sequences=True))(bert_mask)\n",
    "\n",
    "    crf =  CRF(len(cate_dict), sparse_target=True, name = 'crf',learn_mode = 'marginal')(dense1)\n",
    "\n",
    "    #sparse_target=True表示输出可以不用转成one-hot，直接一个向量代表一个句子的bieo序列即可\n",
    "\n",
    "    model = Model(inputs=bert_model.input, outputs = crf)\n",
    "\n",
    "    adam = Adam(lr=5e-06, decay=0.0)\n",
    "    model.compile(adam, loss = crf_loss, metrics=[crf_viterbi_accuracy])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543fad35",
   "metadata": {},
   "source": [
    "# 设置训练方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4366cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_result_id2type(ner_result, netype_dict):\n",
    "    types_lists = []\n",
    "    rows, cols = ner_result.shape\n",
    "    for i in range(rows):\n",
    "        types_list = []\n",
    "        for j in range(cols):\n",
    "            type_ = netype_dict[ner_result[i,j]]\n",
    "#             if type_ == 'null':\n",
    "#                 types_list.append('S')\n",
    "#             else:\n",
    "            types_list.append(type_)\n",
    "        types_lists.append(types_list)\n",
    "    return types_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d058b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import *\n",
    "\n",
    "def predict(model,x_test,y_test):\n",
    "    result = {}\n",
    "    y_pred = model.predict(x_test, batch_size=128)\n",
    "    pred_dev = np.argmax(y_pred,axis = -1)\n",
    "    pred_dev = [[id2label_dict[w] if w != 0 else 'O' for w in s] for s in pred_dev] \n",
    "\n",
    "    true_dev = np.squeeze(y_dev)\n",
    "    true_dev = [[id2label_dict[w] if w != 0 else 'O' for w in s] for s in true_dev] \n",
    "\n",
    "    pred_ens_list = get_ens_list(dev_text_label['text'],pred_dev) \n",
    "    true_ens_list = get_ens_list(dev_text_label['text'],true_dev) \n",
    "\n",
    "    pred_relation_list = get_relations(pred_ens_list)\n",
    "    true_relation_list = get_relations(true_ens_list)\n",
    "\n",
    "    dic,classification_report = Evaluate(Disease_Disease_Label_Mapping.values(),pred_relation_list,true_relation_list)\n",
    "    result['F'] = dic['avg_score']['f1-score']\n",
    "    result['P'] = dic['avg_score']['precision']\n",
    "    result['R'] = dic['avg_score']['recall']\n",
    "    result['classification_report'] = classification_report\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98520751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime,time\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def model_train(projectName,model,crf_numEpoch,x_train,y_train,x_val,y_val):\n",
    "\n",
    "    print(projectName+' Train...')\n",
    "\n",
    "    bestF = -1\n",
    "    now = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    projectPath = './param/subsequence-without-dic-token-level-lc-7-450-40-45-0.7/{}'.format(projectName)\n",
    "    if not os.path.isdir(projectPath): os.makedirs(projectPath)\n",
    "    resultPath = projectPath + '/{}/'.format(now)\n",
    "    os.makedirs(resultPath)\n",
    "\n",
    "    for i in range(0,crf_numEpoch):\n",
    "        epoch_id = 'Epoch '+str(i+1)+\"/\"+str(crf_numEpoch)\n",
    "        print(epoch_id)\n",
    "        hist = model.fit(x_train, y_train, epochs=1, batch_size=36)\n",
    "        result = predict(model,x_val,y_val)\n",
    "        currentF = result['F']\n",
    "        if float(currentF) > bestF:\n",
    "            bestF = float(currentF)\n",
    "            pathOutputModelWeights = resultPath + str(i+1).zfill(3) + \"_\" + str(currentF) +\".txt\"   \n",
    "            model.save_weights(pathOutputModelWeights)\n",
    " \n",
    "    print(bestF)\n",
    "    return pathOutputModelWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01301190",
   "metadata": {},
   "source": [
    "# 设置初始已标注集和未标注集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bda6a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "x_pool = [0]*4\n",
    "x_labeled = [0]*4\n",
    "pool_text_label = {}\n",
    "labeled_text_label = {}\n",
    "for i in range(4):\n",
    "    x_pool[i],x_labeled[i], y_pool, y_labeled = model_selection.train_test_split(x_train[i],y_train,test_size=0.2 ,random_state=1213)\n",
    "pool_text_label['text'], labeled_text_label['text'] = model_selection.train_test_split(train_text_label['text'],test_size=0.2 ,random_state=1213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26ea7340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2273, 128)\n",
      "(2273, 128, 1)\n",
      "(569, 128)\n",
      "(569, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_pool[0].shape)\n",
    "print(y_pool.shape)\n",
    "print(x_labeled[0].shape)\n",
    "print(y_labeled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed9747",
   "metadata": {},
   "source": [
    "# 统计初始已标注集和未标注集中有多少关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2710ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2273it [00:01, 2207.63it/s]\n",
      "569it [00:00, 2252.67it/s]\n"
     ]
    }
   ],
   "source": [
    "true_pool = np.squeeze(y_pool)\n",
    "true_pool = [[id2label_dict[w] if w != 0 else 'O' for w in s] for s in true_pool] \n",
    "true_pool_list = get_ens_list(pool_text_label['text'],true_pool) \n",
    "pool_relation_list = get_relations(true_pool_list)\n",
    "\n",
    "true_labeled = np.squeeze(y_labeled)\n",
    "true_labeled = [[id2label_dict[w] if w != 0 else 'O' for w in s] for s in true_labeled] \n",
    "true_labeled_list = get_ens_list(labeled_text_label['text'],true_labeled) \n",
    "labeled_relation_list = get_relations(true_labeled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d4f0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_relation_count = 0\n",
    "for item in pool_relation_list:\n",
    "    pool_relation_count += len(item)\n",
    "\n",
    "labeled_relation_count = 0\n",
    "for item in labeled_relation_list:\n",
    "    labeled_relation_count += len(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "641a179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3883\n",
      "941\n"
     ]
    }
   ],
   "source": [
    "print(pool_relation_count)\n",
    "print(labeled_relation_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc4419",
   "metadata": {},
   "source": [
    "# 统计初始已标注集和训练集中的关系之间的间隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01187843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2842it [00:01, 2189.19it/s]\n"
     ]
    }
   ],
   "source": [
    "true_train = np.squeeze(y_train)\n",
    "true_train = [[id2label_dict[w] if w != 0 else 'O' for w in s] for s in true_train] \n",
    "true_train_list = get_ens_list(train_text_label['text'],true_train) \n",
    "train_relation_list = get_relations(true_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4e8d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distance(labeled_relation_list, true_labeled_list):\n",
    "    distance_list = []\n",
    "    for index,item in enumerate(labeled_relation_list):\n",
    "        for tmp in item:\n",
    "            subject_ = tmp[0]\n",
    "            object_ = tmp[1]\n",
    "            relation = tmp[2]\n",
    "            subjects = list(filter(lambda x:x[0] == relation + '-1' and x[3] == subject_,true_labeled_list[index]))\n",
    "            objects = list(filter(lambda x:x[0] == relation + '-2' and x[3] == object_,true_labeled_list[index]))\n",
    "            if subjects and objects:\n",
    "                distance = objects[0][2]-subjects[0][1]\n",
    "            distance_list.append(distance)\n",
    "    return distance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6082e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distance_list = count_distance(train_relation_list, true_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc5fbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_distance_list = count_distance(labeled_relation_list, true_labeled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfaedebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distance_list = train_distance_list + labeled_distance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7a9d50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.34258456201214\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(all_distance_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "094d615b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0\n"
     ]
    }
   ],
   "source": [
    "print(np.median(all_distance_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3de9dc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "print(stats.mode(all_distance_list)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ec83c",
   "metadata": {},
   "source": [
    "# 用初始的已标注集训练base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09129ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 128, 768)     1536        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 128, 768)     98304       position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 128, 768)     16151040    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 128, 768)     0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 128, 768)     1536        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 128, 768)     0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask_input (InputLaye (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 128, 2304)    1771776     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bert_attention_mask (BertAttent (None, 1, 128, 128)  0           attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 128, 768)     0           layer_0/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 128, 768)     0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 128, 3072)    0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 128, 768)     0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 128, 768)     0           layer_1/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 128, 768)     0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 128, 3072)    0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 128, 768)     0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 128, 768)     0           layer_2/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 128, 768)     0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 128, 3072)    0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 128, 768)     0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 128, 768)     0           layer_3/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 128, 768)     0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 128, 3072)    0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 128, 768)     0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 128, 768)     0           layer_4/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 128, 768)     0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 128, 3072)    0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 128, 768)     0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 128, 768)     0           layer_5/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 128, 768)     0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 128, 3072)    0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 128, 768)     0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 128, 768)     0           layer_6/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 128, 768)     0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 128, 3072)    0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 128, 768)     0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 128, 768)     0           layer_7/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 128, 768)     0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 128, 3072)    0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 128, 768)     0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 128, 768)     0           layer_8/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 128, 768)     0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 128, 3072)    0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 128, 768)     0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 128, 768)     0           layer_9/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 128, 768)     0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 128, 3072)    0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 128, 768)     0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 128, 2304)    1771776     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 128, 768)     0           layer_10/c_attn[0][0]            \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 128, 768)     590592      layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 128, 768)     0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 128, 768)     0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 128, 768)     1536        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 128, 3072)    2362368     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 128, 3072)    0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 128, 768)     2360064     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 128, 768)     0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 128, 768)     0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 128, 768)     1536        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 128, 2304)    1771776     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 128, 768)     0           layer_11/c_attn[0][0]            \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 128, 768)     590592      layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 128, 768)     0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 128, 768)     0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 128, 768)     1536        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 128, 3072)    2362368     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 128, 3072)    0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 128, 768)     2360064     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 128, 768)     0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 128, 768)     0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 128, 768)     1536        layer_11/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bert_mask_1 (BertMask)          (None, 128, 768)     0           layer_11/ln_2[0][0]              \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128, 768)     590592      bert_mask_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "crf (CRF)                       (None, 128, 26)      20722       dense_1[0][0]                    \n",
      "==================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 101,918,194\n",
      "Trainable params: 101,918,194\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "693it [00:00, 2387.45it/s]\n",
      "693it [00:00, 2349.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'F': '35.91',\n",
       " 'P': '43.61',\n",
       " 'R': '31.52',\n",
       " 'classification_report': '          precision    recall  f1-score   support\\n\\n  C         46.60      37.80      41.74      381\\n  PA        47.93      35.06      40.50      231\\n  RO        22.58      12.00      15.67      175\\n  DD        55.11      45.75      50.00      212\\n  RT        12.50      3.90       5.94       77\\n  RS        77.78      15.91      26.42      44\\n\\navg score     43.61      31.52      35.91      1120'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = bert_crf()\n",
    "# best_model_weights = model_train('base_model_initial',base_model,80,x_labeled,y_labeled,x_dev,y_dev)\n",
    "# base_model.load_weights(best_model_weights)\n",
    "base_model.load_weights('./param/subsequence-active-learning-without-dic/base_model_initial/2021-12-21_15-04-02/070_35.91.txt')\n",
    "predict(base_model,x_dev,y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0b95c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_queries = 7\n",
    "n_instances = 450\n",
    "subsequence_len_range = [40,45]\n",
    "alpha = 0.7\n",
    "dic = {}\n",
    "all_score = {}\n",
    "existed = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3611d08",
   "metadata": {},
   "source": [
    "# 迭代过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7841d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 128, 768)     1536        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 128, 768)     98304       position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 128, 768)     16151040    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 128, 768)     0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 128, 768)     1536        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 128, 768)     0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask_input (InputLaye (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 128, 2304)    1771776     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bert_attention_mask (BertAttent (None, 1, 128, 128)  0           attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 128, 768)     0           layer_0/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 128, 768)     0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 128, 3072)    0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 128, 768)     0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 128, 768)     0           layer_1/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 128, 768)     0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 128, 3072)    0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 128, 768)     0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 128, 768)     0           layer_2/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 128, 768)     0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 128, 3072)    0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 128, 768)     0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 128, 768)     0           layer_3/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 128, 768)     0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 128, 3072)    0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 128, 768)     0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 128, 768)     0           layer_4/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 128, 768)     0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 128, 3072)    0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 128, 768)     0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 128, 768)     0           layer_5/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 128, 768)     0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 128, 3072)    0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 128, 768)     0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 128, 768)     0           layer_6/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 128, 768)     0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 128, 3072)    0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 128, 768)     0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 128, 768)     0           layer_7/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 128, 768)     0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 128, 3072)    0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 128, 768)     0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 128, 768)     0           layer_8/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 128, 768)     0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 128, 3072)    0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 128, 768)     0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 128, 2304)    1771776     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 128, 768)     0           layer_9/c_attn[0][0]             \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 128, 768)     590592      layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 128, 768)     0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 128, 768)     0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 128, 768)     1536        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 128, 3072)    2362368     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 128, 3072)    0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 128, 768)     2360064     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 128, 768)     0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 128, 768)     0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 128, 768)     1536        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 128, 2304)    1771776     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 128, 768)     0           layer_10/c_attn[0][0]            \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 128, 768)     590592      layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 128, 768)     0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 128, 768)     0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 128, 768)     1536        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 128, 3072)    2362368     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 128, 3072)    0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 128, 768)     2360064     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 128, 768)     0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 128, 768)     0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 128, 768)     1536        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 128, 2304)    1771776     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 128, 768)     0           layer_11/c_attn[0][0]            \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 128, 768)     590592      layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 128, 768)     0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 128, 768)     0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 128, 768)     1536        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 128, 3072)    2362368     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 128, 3072)    0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 128, 768)     2360064     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 128, 768)     0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 128, 768)     0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 128, 768)     1536        layer_11/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bert_mask_2 (BertMask)          (None, 128, 768)     0           layer_11/ln_2[0][0]              \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128, 768)     590592      bert_mask_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "crf (CRF)                       (None, 128, 26)      20722       dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 101,918,194\n",
      "Trainable params: 101,918,194\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "base_model_1 Train...\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/caitingting/anaconda3/envs/BERT-Keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/1\n",
      "1019/1019 [==============================] - 23s 23ms/step - loss: 1.3757 - crf_viterbi_accuracy: 0.6728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "693it [00:00, 2331.26it/s]\n",
      "693it [00:00, 2308.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/80\n",
      "Epoch 1/1\n",
      "1019/1019 [==============================] - 13s 13ms/step - loss: 0.6782 - crf_viterbi_accuracy: 0.8243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "693it [00:00, 2305.74it/s]\n",
      "693it [00:00, 2306.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/80\n",
      "Epoch 1/1\n",
      "  36/1019 [>.............................] - ETA: 12s - loss: 0.5162 - crf_viterbi_accuracy: 0.8616"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for query_idx in range(n_queries):\n",
    "    print('Query {n}'.format(n=query_idx + 1))\n",
    "    \n",
    "    y_pool_scores = []\n",
    "    subsequence_scores = []\n",
    "    selected_idx = []\n",
    "    input_id = []\n",
    "    type_id = []\n",
    "    pos_id = []\n",
    "    mask_id = []\n",
    "    y_pool_selected = []\n",
    "    \n",
    "    y_pool_pred = base_model.predict(x_pool,batch_size=128)\n",
    "    \n",
    "    for i in range(y_pool_pred.shape[0]):\n",
    "        tmp = []\n",
    "        x_pool_len = len(np.where(x_pool[0][i]!=0)[0])-2\n",
    "        for j in range(y_pool_pred.shape[1]): \n",
    "            tmp.append(1-math.log(max(y_pool_pred[i][j]),math.e))\n",
    "        y_pool_scores.append(tmp[1:1+x_pool_len])\n",
    "\n",
    "    \n",
    "    for l in range(subsequence_len_range[0],subsequence_len_range[1]+1):\n",
    "        for i in range(len(y_pool_scores)):\n",
    "            for j in range(len(y_pool_scores[i])-l+1):\n",
    "                subsequence_scores.append([i,[j,j+l],sum(y_pool_scores[i][j:j+l])/(l**alpha)])\n",
    "    subsequence_scores = sorted(subsequence_scores,key = lambda x:x[2],reverse = True)\n",
    "\n",
    "    for i in range(len(subsequence_scores)):\n",
    "        union = selected_idx + existed\n",
    "        if [subsequence_scores[i][0],subsequence_scores[i][1]] not in union:\n",
    "            selected_idx_tmp = list(filter(lambda x:x[0] == subsequence_scores[i][0],union))\n",
    "            flag = True\n",
    "            for item in selected_idx_tmp:\n",
    "                if not (subsequence_scores[i][1][0] > item[1][1] or subsequence_scores[i][1][1] < item[1][0]):\n",
    "                    flag = False\n",
    "            if flag:\n",
    "                selected_idx.append([subsequence_scores[i][0],subsequence_scores[i][1]])\n",
    "        if len(selected_idx) == n_instances:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    for item in selected_idx:\n",
    "        sentence_index = item[0]\n",
    "        selected_start = item[1][0]\n",
    "        selected_end = item[1][1]\n",
    "        x_pool_len = len(np.where(x_pool[0][sentence_index]!=0)[0])\n",
    "        if x_pool[0][sentence_index][-1] == 0:\n",
    "            token_tmp = [x_pool[0][sentence_index].tolist()[0]] + x_pool[0][sentence_index].tolist()[selected_start+1:selected_end+1] + [x_pool[0][sentence_index].tolist()[x_pool_len-1]]\n",
    "            label_tmp = [y_pool[sentence_index].tolist()[0]] + y_pool[sentence_index].tolist()[selected_start+1:selected_end+1] + [y_pool[sentence_index].tolist()[x_pool_len-1]]\n",
    "        else:\n",
    "            token_tmp = [x_pool[0][sentence_index].tolist()[0]] + x_pool[0][sentence_index].tolist()[selected_start+1:selected_end+1] + [x_pool[0][sentence_index].tolist()[-1]]\n",
    "            label_tmp = [y_pool[sentence_index].tolist()[0]] + y_pool[sentence_index].tolist()[selected_start+1:selected_end+1] + [y_pool[sentence_index].tolist()[-1]]\n",
    "        token_list = token_tmp + [0]*(max_len-(selected_end-selected_start+2))\n",
    "        label_list = label_tmp + [[0]]*(max_len-(selected_end-selected_start+2))\n",
    "        input_id.append(token_list)\n",
    "        type_id.append([0]*max_len)\n",
    "        pos_id.append([i for i in range(max_len)])\n",
    "        mask_id.append([1]*(selected_end-selected_start+2)+[0]*(max_len-(selected_end-selected_start+2)))\n",
    "        y_pool_selected.append(label_list)\n",
    "\n",
    "    x_pool_selected = [np.array(input_id),np.array(type_id),np.array(pos_id),np.array(mask_id)]\n",
    "    y_pool_selected = np.array(y_pool_selected)\n",
    "    \n",
    "    for t in range(4):\n",
    "        x_labeled[t] = np.concatenate((x_labeled[t],x_pool_selected[t]),axis = 0)    \n",
    "    y_labeled = np.concatenate((y_labeled,y_pool_selected),axis = 0)    \n",
    "\n",
    "\n",
    "    base_model = bert_crf()\n",
    "    best_model_weights = model_train('base_model_' + str(query_idx+1),base_model,80,x_labeled,y_labeled,x_dev,y_dev)\n",
    "    base_model.load_weights(best_model_weights)\n",
    "    result = predict(base_model,x_dev,y_dev)\n",
    "\n",
    "\n",
    "\n",
    "    all_score[str(query_idx+1)] = {\n",
    "        'f1':result['F'],\n",
    "        'recall':result['R'],\n",
    "        'precision':result['P'],\n",
    "        'classification_report':result['classification_report']\n",
    "    }\n",
    "    \n",
    "    print(all_score[str(query_idx+1)])\n",
    "    \n",
    "    existed += selected_idx\n",
    "    \n",
    "#     for a in range(4):\n",
    "#         x_pool[a] = np.delete(x_pool[a], selected_sentence_idx, axis=0)\n",
    "#     y_pool = np.delete(y_pool, selected_sentence_idx, axis=0) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866e487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a08941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
